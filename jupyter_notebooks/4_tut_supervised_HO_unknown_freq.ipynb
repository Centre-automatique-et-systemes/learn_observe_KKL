{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install git+https://github.com/Centre-automatique-et-systemes/lena.git gwpy &> /dev/null\n",
    "!pip3 install git+https://github.com/aliutkus/torchinterp1d.git gwpy &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys ; sys.path.append('../')\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import seaborn as sb\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from learn_KKL.luenberger_observer import LuenbergerObserver\n",
    "from learn_KKL.system import HO_unknown_freq\n",
    "from learn_KKL.learner import Learner\n",
    "\n",
    "sb.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the system\n",
    "system = HO_unknown_freq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "torch.Size([4704, 7])\n",
      "Results saved in in c:\\Users\\Pauline\\Documents\\Pro\\Recherche\\20_LearningTransfoLuenberger\\CodeLukas\\learn_observe_KKL\\jupyter_notebooks\\runs\\HO_unknown_freq\\Supervised/T\\exp_2\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the observer\n",
    "D = torch.diag_embed(torch.tensor([-1.5, -2.0, -3.0, -4.0]))\n",
    "#observer = LuenbergerObserver(dim_x=3, dim_y=1, method='Supervised', wc=0.2,\n",
    "#                              recon_lambda=0.8)\n",
    "observer = LuenbergerObserver(dim_x=3, dim_y=1, method='Supervised', wc=0.2, D=D,\n",
    "                              recon_lambda=0.8)\n",
    "observer.set_dynamics(system)\n",
    "# Generate (x_i, z_i) data by running system backward, then system + observer\n",
    "# forward in time\n",
    "data = observer.generate_data_svl(np.array([[-1, 1.], [-1., 1.], [-1.,1.]]), 7200,\n",
    "                                  method='uniform')\n",
    "print(data.shape)\n",
    "data, val_data = train_test_split(data, test_size=0.3, shuffle=True)\n",
    "\n",
    "# Train the forward transformation using pytorch-lightning and the learner class\n",
    "# Options for training\n",
    "trainer_options={'max_epochs': 15}\n",
    "optimizer_options = {'weight_decay': 1e-6}\n",
    "scheduler_options = {'mode': 'min', 'factor': 0.1, 'patience': 3,\n",
    "                     'threshold': 1e-4, 'verbose': True}\n",
    "stopper = pl.callbacks.early_stopping.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=5e-4, patience=3, verbose=False, mode='min')\n",
    "# Instantiate learner\n",
    "learner_T = Learner(observer=observer, system=system, training_data=data,\n",
    "                    validation_data=val_data, method='T', batch_size=10,\n",
    "                    lr=1e-3, optimizer=optim.Adam,\n",
    "                    optimizer_options=optimizer_options,\n",
    "                    scheduler=optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                    scheduler_options=scheduler_options)\n",
    "# Define logger and checkpointing\n",
    "logger = TensorBoardLogger(save_dir=learner_T.results_folder + '/tb_logs')\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss')\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=[stopper, checkpoint_callback], **trainer_options, logger=logger,\n",
    "    log_every_n_steps=1, check_val_every_n_epoch=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "  | Name  | Type               | Params\n",
      "---------------------------------------------\n",
      "0 | model | LuenbergerObserver | 26.3 K\n",
      "---------------------------------------------\n",
      "26.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "26.3 K    Total params\n",
      "0.105     Total estimated model params size (MB)\n",
      "Logs stored in c:\\Users\\Pauline\\Documents\\Pro\\Recherche\\20_LearningTransfoLuenberger\\CodeLukas\\learn_observe_KKL\\jupyter_notebooks\\runs\\HO_unknown_freq\\Supervised/T\\exp_2/tb_logs\n",
      "Epoch 2:  70%|██████▉   | 330/472 [00:02<00:01, 140.58it/s, loss=4.63e-05, v_num=0, train_loss=2.96e-5]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2:  83%|████████▎ | 393/472 [00:02<00:00, 160.64it/s, loss=4.63e-05, v_num=0, train_loss=2.96e-5]\n",
      "Epoch 2:  98%|█████████▊| 461/472 [00:02<00:00, 181.03it/s, loss=4.63e-05, v_num=0, train_loss=2.96e-5]\n",
      "Epoch 2: 100%|██████████| 472/472 [00:02<00:00, 183.20it/s, loss=4.63e-05, v_num=0, train_loss=2.96e-5, val_loss_step=8.32e-6, val_loss_epoch=2.01e-5]\n",
      "Epoch 5:  70%|██████▉   | 330/472 [00:02<00:01, 132.30it/s, loss=1.23e-05, v_num=0, train_loss=1.49e-5, val_loss_step=8.32e-6, val_loss_epoch=2.01e-5]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5:  72%|███████▏  | 340/472 [00:02<00:00, 134.79it/s, loss=1.23e-05, v_num=0, train_loss=1.49e-5, val_loss_step=8.32e-6, val_loss_epoch=2.01e-5]\n",
      "Epoch 5:  86%|████████▋ | 408/472 [00:02<00:00, 153.92it/s, loss=1.23e-05, v_num=0, train_loss=1.49e-5, val_loss_step=8.32e-6, val_loss_epoch=2.01e-5]\n",
      "Epoch 5: 100%|██████████| 472/472 [00:02<00:00, 171.18it/s, loss=1.23e-05, v_num=0, train_loss=1.49e-5, val_loss_step=3.23e-5, val_loss_epoch=1.24e-5]\n",
      "Epoch 8:  70%|██████▉   | 330/472 [00:02<00:01, 141.55it/s, loss=1.49e-05, v_num=0, train_loss=1.8e-6, val_loss_step=3.23e-5, val_loss_epoch=1.24e-5] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8:  72%|███████▏  | 340/472 [00:02<00:00, 144.18it/s, loss=1.49e-05, v_num=0, train_loss=1.8e-6, val_loss_step=3.23e-5, val_loss_epoch=1.24e-5]\n",
      "Epoch 8:  86%|████████▋ | 408/472 [00:02<00:00, 165.40it/s, loss=1.49e-05, v_num=0, train_loss=1.8e-6, val_loss_step=3.23e-5, val_loss_epoch=1.24e-5]\n",
      "Epoch 8: 100%|██████████| 472/472 [00:02<00:00, 184.02it/s, loss=1.49e-05, v_num=0, train_loss=1.8e-6, val_loss_step=2.23e-6, val_loss_epoch=6.67e-6]\n",
      "Epoch 11:  70%|██████▉   | 330/472 [00:02<00:01, 140.26it/s, loss=1.12e-05, v_num=0, train_loss=1.13e-5, val_loss_step=2.23e-6, val_loss_epoch=6.67e-6]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11:  72%|███████▏  | 340/472 [00:02<00:00, 143.01it/s, loss=1.12e-05, v_num=0, train_loss=1.13e-5, val_loss_step=2.23e-6, val_loss_epoch=6.67e-6]\n",
      "Epoch 11:  86%|████████▋ | 408/472 [00:02<00:00, 164.02it/s, loss=1.12e-05, v_num=0, train_loss=1.13e-5, val_loss_step=2.23e-6, val_loss_epoch=6.67e-6]\n",
      "Epoch 11: 100%|██████████| 472/472 [00:02<00:00, 182.49it/s, loss=1.12e-05, v_num=0, train_loss=1.13e-5, val_loss_step=0.000181, val_loss_epoch=3.54e-5]\n",
      "Epoch 11: 100%|██████████| 472/472 [00:02<00:00, 182.21it/s, loss=1.12e-05, v_num=0, train_loss=1.13e-5, val_loss_step=0.000181, val_loss_epoch=3.54e-5]\n",
      "Saved model in c:\\Users\\Pauline\\Documents\\Pro\\Recherche\\20_LearningTransfoLuenberger\\CodeLukas\\learn_observe_KKL\\jupyter_notebooks\\runs\\HO_unknown_freq\\Supervised/T\\exp_2\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# To see logger in tensorboard, copy the following output name_of_folder\n",
    "print(f'Logs stored in {learner_T.results_folder}/tb_logs')\n",
    "# which should be similar to jupyter_notebooks/runs/method/exp_0/tb_logs/\n",
    "# Then type this in terminal:\n",
    "# tensorboard --logdir=name_of_folder --port=8080\n",
    "\n",
    "# Train and save results\n",
    "trainer.fit(learner_T)\n",
    "learner_T.save_results(limits=np.array([[-1, 1.], [-1., 1.], [-1., 1.]]),\n",
    "                       nb_trajs=10, tsim=(0, 10), dt=1e-4,\n",
    "                       checkpoint_path=checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "Results saved in in c:\\Users\\Pauline\\Documents\\Pro\\Recherche\\20_LearningTransfoLuenberger\\CodeLukas\\learn_observe_KKL\\jupyter_notebooks\\runs\\HO_unknown_freq\\Supervised/T_star\\exp_3\n"
     ]
    }
   ],
   "source": [
    "# Train the inverse transformation using pytorch-lightning and the learner class\n",
    "# Options for training\n",
    "trainer_options={'max_epochs': 20}\n",
    "optimizer_options = {'weight_decay': 1e-8}\n",
    "scheduler_options = {'mode': 'min', 'factor': 0.1, 'patience': 3,\n",
    "                     'threshold': 1e-4, 'verbose': True}\n",
    "stopper = pl.callbacks.early_stopping.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=5e-4, patience=3, verbose=False, mode='min')\n",
    "# Instantiate learner\n",
    "learner_T_star = Learner(observer=observer, system=system, training_data=data,\n",
    "                         validation_data=val_data, method='T_star',\n",
    "                         batch_size=10, lr=5e-4, optimizer=optim.Adam,\n",
    "                         optimizer_options=optimizer_options,\n",
    "                         scheduler=optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                         scheduler_options=scheduler_options)\n",
    "# Define logger and checkpointing\n",
    "logger = TensorBoardLogger(save_dir=learner_T_star.results_folder + '/tb_logs')\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss')\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=[stopper, checkpoint_callback], **trainer_options, logger=logger,\n",
    "    log_every_n_steps=1, check_val_every_n_epoch=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "  | Name  | Type               | Params\n",
      "---------------------------------------------\n",
      "0 | model | LuenbergerObserver | 26.3 K\n",
      "---------------------------------------------\n",
      "26.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "26.3 K    Total params\n",
      "0.105     Total estimated model params size (MB)\n",
      "Logs stored in c:\\Users\\Pauline\\Documents\\Pro\\Recherche\\20_LearningTransfoLuenberger\\CodeLukas\\learn_observe_KKL\\jupyter_notebooks\\runs\\HO_unknown_freq\\Supervised/T_star\\exp_3/tb_logs\n",
      "Epoch 2:  70%|██████▉   | 330/472 [00:02<00:01, 118.77it/s, loss=0.36, v_num=0, train_loss=0.371] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2:  73%|███████▎  | 345/472 [00:02<00:01, 122.61it/s, loss=0.36, v_num=0, train_loss=0.371]\n",
      "Epoch 2:  86%|████████▌ | 407/472 [00:02<00:00, 139.67it/s, loss=0.36, v_num=0, train_loss=0.371]\n",
      "Epoch 2: 100%|██████████| 472/472 [00:03<00:00, 156.30it/s, loss=0.36, v_num=0, train_loss=0.371, val_loss_step=0.346, val_loss_epoch=0.367]\n",
      "Epoch 5:  70%|██████▉   | 330/472 [00:02<00:01, 121.08it/s, loss=0.372, v_num=0, train_loss=0.284, val_loss_step=0.346, val_loss_epoch=0.367]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/142 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  83%|████████▎ | 390/472 [00:02<00:00, 137.65it/s, loss=0.372, v_num=0, train_loss=0.284, val_loss_step=0.346, val_loss_epoch=0.367]\n",
      "Epoch 5: 100%|██████████| 472/472 [00:02<00:00, 159.01it/s, loss=0.372, v_num=0, train_loss=0.284, val_loss_step=0.591, val_loss_epoch=0.367]\n",
      "Epoch 8:  70%|██████▉   | 330/472 [00:02<00:01, 118.85it/s, loss=0.375, v_num=0, train_loss=0.536, val_loss_step=0.591, val_loss_epoch=0.367]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/142 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  83%|████████▎ | 390/472 [00:02<00:00, 135.10it/s, loss=0.375, v_num=0, train_loss=0.536, val_loss_step=0.591, val_loss_epoch=0.367]\n",
      "Epoch 8: 100%|██████████| 472/472 [00:03<00:00, 156.65it/s, loss=0.375, v_num=0, train_loss=0.536, val_loss_step=0.146, val_loss_epoch=0.367]\n",
      "                                                              \u001b[AEpoch     9: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch 11:  70%|██████▉   | 330/472 [00:02<00:01, 123.24it/s, loss=0.337, v_num=0, train_loss=0.367, val_loss_step=0.146, val_loss_epoch=0.367]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11:  81%|████████  | 380/472 [00:02<00:00, 137.50it/s, loss=0.337, v_num=0, train_loss=0.367, val_loss_step=0.146, val_loss_epoch=0.367]\n",
      "Validating:  43%|████▎     | 61/142 [00:00<00:00, 609.72it/s]\u001b[A\n",
      "Epoch 11: 100%|██████████| 472/472 [00:02<00:00, 161.61it/s, loss=0.337, v_num=0, train_loss=0.367, val_loss_step=0.0932, val_loss_epoch=0.367]\n",
      "Epoch 11: 100%|██████████| 472/472 [00:02<00:00, 161.33it/s, loss=0.337, v_num=0, train_loss=0.367, val_loss_step=0.0932, val_loss_epoch=0.367]\n",
      "Saved model in c:\\Users\\Pauline\\Documents\\Pro\\Recherche\\20_LearningTransfoLuenberger\\CodeLukas\\learn_observe_KKL\\jupyter_notebooks\\runs\\HO_unknown_freq\\Supervised/T_star\\exp_3\n",
      "No handles with labels found to put in legend.\n",
      "Shape of mesh for evaluation: torch.Size([6534, 7])\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n"
     ]
    }
   ],
   "source": [
    "# To see logger in tensorboard, copy the following output name_of_folder\n",
    "print(f'Logs stored in {learner_T_star.results_folder}/tb_logs')\n",
    "# which should be similar to jupyter_notebooks/runs/method/exp_0/tb_logs/\n",
    "# Then type this in terminal:\n",
    "# tensorboard --logdir=name_of_folder --port=8080\n",
    "\n",
    "# Train and save results\n",
    "trainer.fit(learner_T_star)\n",
    "learner_T_star.save_results(limits=np.array([[-1, 1.], [-1., 1.], [-1.,1.]]),\n",
    "                            nb_trajs=10, tsim=(0, 10), dt=1e-4,\n",
    "                            checkpoint_path=checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.0716)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# We compare the learned T to the theoretical T\n",
    "# T(x) = C (A(x_3)-lambda I)^{-1} [x_1;x_2]\n",
    "# with C = [1,0] and A(x_3) = [0, 1; -x_3, 0]\n",
    "# (this is for a diagonal D and F = ones)\n",
    "\n",
    "N = learner_T.validation_data.size()[0]\n",
    "T_theo = torch.zeros(N,4)\n",
    "lambda_D = torch.diag(D)\n",
    "A = torch.tensor([[0.,1.],[-learner_T.validation_data[17,2],0.]])\n",
    "C = torch.tensor([[1.,0.]])\n",
    "x12 = learner_T.validation_data[17,:2]\n",
    "for ind in range(N):\n",
    "    A = torch.tensor([[0.,1.],[-learner_T.validation_data[ind,2],0.]])\n",
    "    x12 = learner_T.validation_data[ind,:2]\n",
    "    #print(ind)\n",
    "    for ind_eig in range(4):\n",
    "        #print(torch.linalg.eig(A-lambda_D[ind_eig]*torch.eye(2)))\n",
    "        T_theo[ind,ind_eig] =  C@torch.linalg.inv(A-lambda_D[ind_eig]*torch.eye(2))@x12\n",
    "\n",
    "\n",
    "error = abs(T_theo-learner_T.validation_data[:,3:])\n",
    "print(error.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "876800e5f4140bb8c9eb7cb3630e01ad622f0981781552d39a63585948556d04"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('numericalKKL': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8623a79dd160377ae64a232bb47a6eaa7c3991c99ffbac80aed770bfffe00916"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}